# Speedrun-MuP ðŸš€

A clean experimental stack for **Maximal Update Parameterization (Î¼P)** scaling experiments built on top of the modded-nanogpt "speedrun" architecture.

## Overview

Speedrun-MuP combines the world-record training speed of modded-nanogpt with the principled scaling laws of Î¼P to enable:

- **Width-invariant hyperparameters** that transfer across model scales
- **Competitive training speed** with modern optimizations (FlexAttention, FP8, Muon optimizer)
- **Comprehensive validation** through coordinate checking and spectral monitoring
- **Research-ready metrics** with automatic plotting and W&B integration

  speedrun_mup/
  â”œâ”€â”€ core/                    # Clean, concise implementation
  â”‚   â”œâ”€â”€ __init__.py         # Simple package initialization
  â”‚   â”œâ”€â”€ model.py            # GPT implementation following modded-nanogpt
  â”‚   â”œâ”€â”€ mup.py              # MuP scaling and coordinate checking
  â”‚   â””â”€â”€ utils.py            # Consolidated logging and utilities
  â”œâ”€â”€ scripts/                # Executable training scripts
  â”‚   â”œâ”€â”€ train.py           # Main training script with MuP
  â”‚   â””â”€â”€ coord_check.py     # Coordinate validation
  â”œâ”€â”€ configs/                # Simplified YAML configurations
  â”‚   â”œâ”€â”€ gpt_small_mup.yaml # Basic GPT-small config
  â”‚   â””â”€â”€ width_sweep.yaml   # Width experiments config
  â”œâ”€â”€ claude_instructions/    # Preserved unchanged
  â””â”€â”€ origin_repos/          # Preserved unchanged
      â”œâ”€â”€ modded-nanogpt/
      â””â”€â”€ mup/

## Quick Start

### 1. Coordinate Checking

First, validate your MuP implementation:

```bash
python scripts/coord_check.py \
    --mup-config configs/mup/width_sweep.yaml \
    --widths 256 512 1024 \
    --output-dir ./coord_check_results
```

This will:
- Test models at different widths
- Generate coordinate check plots
- Validate that activations remain O(1) across widths

### 2. Basic Training

Run a basic MuP training experiment:

```bash
python scripts/train.py \
    --config configs/base/gpt_small.yaml \
    --mup-config configs/mup/width_sweep.yaml \
    --run-name my_mup_experiment
```

### 3. Width Scaling Experiment

Test hyperparameter transfer across multiple widths:

```bash
# Train base model
python scripts/train.py \
    --config configs/base/gpt_small.yaml \
    --mup-config configs/mup/width_sweep.yaml \
    --run-name base_width_256

# Transfer to larger widths (automatically uses MuP scaling)
python scripts/train.py \
    --config configs/base/gpt_medium.yaml \
    --mup-config configs/mup/width_sweep.yaml \
    --run-name transferred_width_768
```

## Project Structure

```
speedrun_mup/
â”œâ”€â”€ speedrun_mup/           # Core package
â”‚   â”œâ”€â”€ models/            # MuP-aware model implementations
â”‚   â”œâ”€â”€ training/          # Training loops and optimizers
â”‚   â”œâ”€â”€ config/            # Configuration management
â”‚   â”œâ”€â”€ logging/           # Metrics and W&B integration
â”‚   â”œâ”€â”€ validation/        # Coordinate checking and validation
â”‚   â””â”€â”€ utils/             # MuP utilities and shape management
â”œâ”€â”€ configs/               # Experiment configurations
â”‚   â”œâ”€â”€ base/             # Base model configurations
â”‚   â”œâ”€â”€ mup/              # MuP-specific configurations
â”‚   â””â”€â”€ experiments/      # Full experiment suites
â”œâ”€â”€ scripts/              # Entry point scripts
â”‚   â”œâ”€â”€ train.py         # Main training script
â”‚   â”œâ”€â”€ coord_check.py   # Coordinate checking
â”‚   â””â”€â”€ analyze_run.py   # Post-training analysis
â””â”€â”€ analysis/             # Notebooks and plotting utilities
```


## Troubleshooting

### Coordinate Checks Fail
```bash
# Common issues:
# 1. Incorrect base shapes - regenerate with fresh models
# 2. Architecture incompatibility - check skip connections
# 3. Initialization problems - verify MuP init is applied

# Debug with:
python scripts/coord_check.py --widths 256 512 --n-steps 5
```

## Recommand links (kexue.fm is all you need)

- [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)
- [Î¼P Practitioner's Guide](https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization)
- [Modded-NanoGPT Repository](https://github.com/KellerJordan/modded-nanogpt)
- [Higher-order Î¼P Spectral Conditions](https://kexue.fm/archives/10795)

---
