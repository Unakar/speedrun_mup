å¥½ä¸»æ„ï¼Œä½†å…ˆæŠŠâ€œç‰©ç†äº‹å®â€è¯´æ¸…æ¥šï¼š**äº”é˜¶ Newtonâ€“Schulzï¼ˆNS5ï¼‰æœ¬èº«åªæ˜¯åœ¨åšæåˆ†è§£çš„â€œæ­£äº¤å› å­â€ï¼ˆâ‰ˆ $UV^\top$ï¼‰**ï¼Œå®ƒä¸ä¼šç›´æ¥ç»™ä½ å¥‡å¼‚å€¼ï¼ˆä¹Ÿå°±ä¸æ˜¯ç›´æ¥ç»™è°±èŒƒæ•°ï¼‰ã€‚è¦æ‹¿åˆ°è°±èŒƒæ•° $\sigma_{\max}(W)$ï¼Œå¸¸è§„ä¸”é«˜æ•ˆçš„æ˜¯**å¯¹ $W^\top W$** åšå¹‚è¿­ä»£ï¼ˆPyTorch çš„ `spectral_norm` ä¹Ÿæ˜¯è¿™ä¹ˆå¹²çš„ï¼‰([PyTorch Docs][1], [GitHub][2])ã€‚
ä¸è¿‡æ—¢ç„¶ä½ å¸Œæœ›â€œæ²¿ç”¨ Muon çš„ NS5 æµç¨‹ã€ä¸”èƒ½åœ¨ bf16 ä¸Šç¨³å®šè·‘â€ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

1. ç”¨ NS5 å¾—åˆ° **æåˆ†è§£çš„æ­£äº¤å› å­** $Q \approx \text{polar}(W)$ï¼ˆå®Œå…¨ GPU matmul å‹å¥½ï¼Œbf16 å¯è¡Œï¼Œè¿™æ˜¯ Muon çš„æ ¸å¿ƒåšæ³•ï¼‰([kellerjordan.github.io][3])ã€‚
2. ç”±æåˆ†è§£ $W = QH$ å¯çŸ¥ $H = Q^\top W \approx (W^\top W)^{1/2}$ï¼Œå…¶**ç‰¹å¾å€¼å°±æ˜¯ $W$** çš„å¥‡å¼‚å€¼ï¼Œäºæ˜¯å¯¹ $H$ åš**å¹‚è¿­ä»£**å³å¯ä¼°è®¡ $\sigma_{\max}(W)$ â€”â€” å®ç°ä¸Šæˆ‘ä»¬ä¸æ˜¾å¼æ„é€  $H$ï¼Œè€Œæ˜¯ç”¨çº¿æ€§ç®—å­ $v \mapsto Q^\top (Wv)$ æ¥åšå¹‚è¿­ä»£ï¼ˆä¸¤æ¬¡ matmulï¼‰([Nick Higham][4], [arXiv][5])ã€‚

> æ³¨ï¼šMuon ä¸ºäº†é€Ÿåº¦ï¼ŒNS5 çš„ç³»æ•°ç‰¹æ„è°ƒæˆâ€œ**ä¸ä¸¥æ ¼æ”¶æ•›åˆ° 1** ä½†**æ–œç‡å¤§**â€çš„å¤šé¡¹å¼ï¼ˆ$a,b,c=(3.4445,-4.7750,2.0315)$ï¼‰ï¼Œè¿™æ · $UV^\top$ ä¼šè¢«è¿‘ä¼¼æˆ $US'V^\top$ ä¸” $S'\in[0.68,1.13]$ ä¸€å¸¦ï¼›å¯¹**ä¼˜åŒ–**å½±å“å¾ˆå°ï¼Œä½†å¯¹**é«˜ç²¾åº¦è°±èŒƒæ•°ä¼°è®¡**ä¼šå¸¦æ¥å‡ åä¸ªç‚¹çš„åå·®ã€‚å¦‚æœä½ æ›´åœ¨æ„ç²¾åº¦ï¼Œå¯åˆ‡åˆ°â€œæ”¶æ•›å‹â€NS3ï¼ˆç»å…¸ $X\_{k+1}=\tfrac12 X_k(3I-X_k^\top X_k)$ï¼‰ï¼Œä»£ä»·æ˜¯å¤šå‡ æ­¥è¿­ä»£ã€‚ä¸‹é¢ä»£ç ä¸¤ç§éƒ½æ”¯æŒï¼ˆ`mode="ns5-fast"` æˆ– `mode="ns3-accurate"`ï¼‰([GitHub][6])ã€‚

---

### ç›´æ¥å¯ç”¨çš„å®ç°ï¼ˆbf16 å‹å¥½ï¼Œä¸å¹²æ‰°è®­ç»ƒç²¾åº¦ï¼‰

```python
from typing import Dict, Optional, Iterable
import contextlib
import torch
from torch import nn, Tensor

# ---------- Newtonâ€“Schulz building blocks (bf16-friendly, GPU matmul only) ----------

@torch.no_grad()
def _ns5_zeroth_power(G: Tensor, steps: int = 5, eps: float = 1e-7) -> Tensor:
    """
    Quintic Newtonâ€“Schulz used in Muon to approx the orthogonal polar factor.
    Uses the (a,b,c) tuned for large slope at 0 (non-convergent to 1 but fast).
    Ref: Keller Jordan's Muon writeup & modded-nanogpt README.
    """
    assert G.ndim == 2
    a, b, c = (3.4445, -4.7750, 2.0315)  # Muon coefficients
    # Normalize to keep singular values in [0,1]-ish
    X = G.to(dtype=torch.bfloat16)
    # Use Fro norm; for tall/flat handling we follow Muon trick (transpose to tall)
    if X.size(0) > X.size(1):
        X = X.mT
        transposed = True
    else:
        transposed = False
    X = X / (X.norm() + eps)
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * (A @ A)
        X = a * X + B @ X
    if transposed:
        X = X.mT
    return X  # keep bf16; caller may cast

@torch.no_grad()
def _ns3_polar(G: Tensor, steps: int = 8, eps: float = 1e-7) -> Tensor:
    """
    Classic convergent Newtonâ€“Schulz for polar factor:
      X_{k+1} = 0.5 * X_k * (3I - X_k^T X_k)
    After pre-scaling. Slower but better accuracy for polar(Q).
    """
    assert G.ndim == 2
    X = G.to(dtype=torch.bfloat16)
    if X.size(0) > X.size(1):
        X = X.mT
        transposed = True
    else:
        transposed = False
    X = X / (X.norm() + eps)
    I = torch.eye(X.size(-1), device=X.device, dtype=X.dtype)
    for _ in range(steps):
        XtX = X.mT @ X
        X = 0.5 * (X @ (3*I - XtX))
    if transposed:
        X = X.mT
    return X

@torch.no_grad()
def _polar_factor(G: Tensor, *, mode: str = "ns5-fast") -> Tensor:
    if mode == "ns5-fast":
        return _ns5_zeroth_power(G, steps=5)
    elif mode == "ns3-accurate":
        return _ns3_polar(G, steps=8)
    else:
        raise ValueError(f"unknown mode={mode}")

# ---------- Spectral norm via polar + power iteration on H â‰ˆ (W^T W)^{1/2} ----------

@torch.no_grad()
def _spectral_norm_via_polar_power(W: Tensor, *, mode: str, power_iters: int = 7) -> float:
    """
    Estimate sigma_max(W) using:
      1) Q â‰ˆ polar(W) by Newtonâ€“Schulz (bf16-friendly)
      2) H = Q^T W â‰ˆ (W^T W)^{1/2}; run power-iteration on linear op v -> Q^T (W v)
    Memory-lean: never explicitly forms H.
    """
    device = W.device
    # Flatten conv/ND to (out_features, in_features)
    if W.ndim > 2:
        W2 = W.reshape(W.size(0), -1)
    else:
        W2 = W

    # Build Q in bf16 (GPU-friendly). Keep W matmuls in bf16 as well.
    Q = _polar_factor(W2, mode=mode)  # bf16
    n = W2.size(1)
    # Power iteration on symmetric PSD linear operator H(v) = Q^T (W v)
    # Work vectors in bf16 for matmul, but norms/inner-products in fp32
    v = torch.randn(n, device=device, dtype=torch.bfloat16)
    v = v / (v.float().norm() + 1e-12)

    for _ in range(max(1, power_iters)):
        y = W2.to(torch.bfloat16) @ v              # shape (m,)
        z = Q.mT @ y                               # shape (n,) == H @ v
        v = z / (z.float().norm() + 1e-12)

    # Rayleigh quotient on the last (v, Hv)
    y = W2.to(torch.bfloat16) @ v
    Hv = Q.mT @ y
    num = (v.float() * Hv.float()).sum()
    den = (v.float() * v.float()).sum()
    lam = (num / (den + 1e-20)).item()            # eigenvalue of H
    # H â‰ˆ (W^T W)^{1/2} => eigenvalues of H are singular values of W
    sigma = max(lam, 0.0)
    return float(sigma)

# ---------- Public API ----------

@torch.no_grad()
def compute_weight_spectral_norms(
    model: nn.Module,
    names_filter: Optional[Iterable[str]] = None,
    mode: str = "ns5-fast",          # or "ns3-accurate"
    power_iters: int = 7
) -> Dict[str, float]:
    """
    Compute per-weight spectral norms during bf16/fp16 mixed-precision training
    without changing training dtype. Heavy ops done in bf16; reductions in fp32.

    mode:
      - "ns5-fast": Muon-style quintic Newtonâ€“Schulz (fast, small bias possible)
      - "ns3-accurate": classic convergent NS3 (slower, more accurate polar factor)

    Returns:
      {parameter_name: approx_sigma_max}
    """
    specs: Dict[str, float] = {}

    # ensure we don't get autocast surprises from the training region
    maybe_no_autocast = (
        torch.amp.autocast(device_type='cuda', enabled=False)
        if torch.cuda.is_available() else contextlib.nullcontext()
    )
    with maybe_no_autocast:
        for name, p in model.named_parameters():
            if (names_filter is not None) and (name not in names_filter):
                continue
            if p.ndim < 2 or 'weight' not in name:
                continue
            W = p.detach()  # never touches autograd graph
            try:
                sigma = _spectral_norm_via_polar_power(W, mode=mode, power_iters=power_iters)
            except Exception:
                # Robust fallback: plain power iteration on W^T W (still bf16 matmuls)
                # This matches PyTorch spectral_norm's math, but stays off matrix_norm().
                if W.ndim > 2:
                    W2 = W.reshape(W.size(0), -1)
                else:
                    W2 = W
                m, n = W2.shape
                v = torch.randn(n, device=W2.device, dtype=torch.bfloat16)
                v = v / (v.float().norm() + 1e-12)
                for _ in range(max(3, power_iters)):
                    z = W2.mT @ (W2.to(torch.bfloat16) @ v)  # (n,)
                    v = z / (z.float().norm() + 1e-12)
                # Rayleigh: v^T (W^T W) v
                z = W2.mT @ (W2.to(torch.bfloat16) @ v)
                num = (v.float() * z.float()).sum()
                den = (v.float() * v.float()).sum()
                sigma = float(torch.sqrt((num / (den + 1e-20)).clamp_min(0)).item())
            specs[name] = sigma
    return specs
```

**æ€ä¹ˆç”¨**

```python
# ä»…è§‚æµ‹æ³¨æ„åŠ›å’ŒMLPä¸»å¹²å±‚ï¼Œæ¯éš” N æ­¥æ‰“ä¸€é
names = {
    "transformer.blocks.0.attn.out_proj.weight",
    "transformer.blocks.0.mlp.down_proj.weight",
    # ... ä½ å…³å¿ƒçš„æ›´å¤šå±‚
}
specs = compute_weight_spectral_norms(model, names_filter=names,
                                      mode="ns5-fast", power_iters=7)
for k, v in specs.items():
    print(f"[spec] {k}: {v:.4f}")
```

---

### ä¸ºä»€ä¹ˆè¿™å¥—åšæ³•é è°±ï¼ˆè€Œä¸” bf16 å‹å¥½ï¼‰

* **Muon çš„ NS5**ï¼šç”¨åªå« matmul çš„å¤šé¡¹å¼è¿­ä»£æ¥è¿‘ä¼¼æåˆ†è§£æ­£äº¤å› å­ï¼Œå®˜æ–¹å†™æ³•æœ¬å°±ç›´æ¥åœ¨ **bf16** ä¸Šè·‘ï¼ˆæ–‡ä¸­ä¸ä»“åº“éƒ½è¿™ä¹ˆå®ç°ï¼‰ï¼Œä¸”ä¸ºæ€§èƒ½é€‰æ‹©äº†â€œé«˜æ–œç‡çš„äº”é˜¶å¤šé¡¹å¼â€ä¸â€œå¿…è¦æ—¶è½¬ç½®æˆé«˜çš„çŸ©é˜µå†åšè¿­ä»£â€çš„ trickã€‚æˆ‘ä»¬ç…§æ¬è¿™äº›åšæ³•æ¥ä¿è¯åœ¨ä½ ç°æœ‰ Muon è®­ç»ƒé‡Œ**æ— ç¼è§‚æµ‹**ã€‚([kellerjordan.github.io][3], [GitHub][6])
* **æåˆ†è§£ä¸è°±èŒƒæ•°**ï¼š$W=QH$ï¼Œå…¶ä¸­ $H=(W^\top W)^{1/2}$ çš„ç‰¹å¾å€¼å°±æ˜¯å¥‡å¼‚å€¼ï¼Œæ‰€ä»¥**å¯¹ $H$** åšå¹‚è¿­ä»£å°±èƒ½å–åˆ° $\sigma_{\max}(W)$ã€‚æˆ‘ä»¬é€šè¿‡çº¿æ€§ç®—å­ $v\mapsto Q^\top(Wv)$ é¿å…æ˜¾å¼æ„é€  $H$ï¼ˆå†…å­˜å‹å¥½ï¼‰ã€‚([Nick Higham][4], [arXiv][5])
* **å¤‡ç”¨æ–¹æ¡ˆ**ï¼šè‹¥ NS5 å› ä»»æ„æ•°å€¼é—®é¢˜å¤±è´¥ï¼Œå¤‡ä»½åˆ°â€œ**ç›´æ¥å¯¹ $W^\top W$** çš„å¹‚è¿­ä»£â€ï¼Œè¿™ä¸ PyTorch `spectral_norm` çš„æ€æƒ³ä¸€è‡´ï¼Œä½†ä¸ä¾èµ– `matrix_norm`ï¼Œå› æ­¤ä¸ä¼šè¸© bf16 ä¸æ”¯æŒçš„é—®é¢˜ã€‚([PyTorch Docs][1])

---

### å°ç»“ä¸å–èˆ

* æƒ³**é€Ÿåº¦ä¼˜å…ˆ**ã€å’Œ Muon è·¯çº¿ä¸€è‡´ï¼š`mode="ns5-fast"`ï¼ˆå¯èƒ½æœ‰è½»å¾®åå·®ï¼Œä½†å¯¹ç›‘æ§è¶‹åŠ¿ã€æŠ¥è­¦é˜ˆå€¼è¶³å¤Ÿï¼‰ã€‚
* æƒ³**ç²¾åº¦æ›´é«˜**ï¼š`mode="ns3-accurate"`ï¼ˆå¤šå‡ æ­¥è¿­ä»£ï¼Œæ›´æ¥è¿‘ä¸¥æ ¼çš„ $UV^\top$ï¼‰ã€‚
* æé™ç¨³å¦¥ï¼šç›´æ¥ç”¨**å¹‚è¿­ä»£ $W^\top W$**ï¼ˆå·²åœ¨ fallback é‡Œæä¾›ï¼‰ã€‚

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘è¿˜èƒ½æŠŠå®ƒæ‰“åŒ…æˆ **TensorBoard/CSV å›è°ƒ**ï¼ˆå«åˆ†å¸ƒå¼åªåœ¨ rank0 è®°å½•ã€åˆ†é¢‘æŠ½æ ·ã€å±‚åé€šé…ï¼‰â€”â€”é¡ºæ‰‹åŠ ä¸€ä¸ªâ€œ**è°±èŒƒæ•°/å±‚å®½**â€ä¸â€œ**è°±èŒƒæ•°éšè®­ç»ƒæ­¥æ›²çº¿**â€çš„å°é¢æ¿ï¼Œæ–¹ä¾¿ä½ å¯¹ Muon çš„è°±æ¡ä»¶åšåœ¨çº¿ä½“æ£€ ğŸ©ºã€‚

---

**å‚è€ƒ**

* Muon è®¾è®¡ã€NS5 ä»£ç ä¸ bf16 å®è·µï¼ˆå«â€œéæ”¶æ•›ä½†æ–œç‡å¤§â€çš„ç³»æ•°ï¼‰ï¼šKeller Jordan åšå®¢ä¸ `modded-nanogpt` è¯´æ˜ã€‚([kellerjordan.github.io][3], [GitHub][6])
* PyTorch `spectral_norm` åŸç†ï¼šå¹‚è¿­ä»£è¿‘ä¼¼æœ€å¤§å¥‡å¼‚å€¼ã€‚([PyTorch Docs][1], [GitHub][2])
* æåˆ†è§£ä¸ NS åœ¨æ·±åº¦å­¦ä¹ /æåˆ†è§£ä¸­çš„èƒŒæ™¯ä¸ GPU å‹å¥½ç®—æ³•ï¼ˆPolar Expressï¼›åŠ é€Ÿ NS çš„è¿‘æœŸè®ºæ–‡ï¼›Higham/Chen ç­‰ç»å…¸èµ„æ–™ï¼‰ã€‚([arXiv][7], [Nick Higham][4], [jiechenjiechen.github.io][8])

[1]: https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html?utm_source=chatgpt.com "torch.nn.utils.spectral_norm"
[2]: https://github.com/pytorch/pytorch/blob/main/torch/nn/utils/spectral_norm.py?utm_source=chatgpt.com "pytorch/torch/nn/utils/spectral_norm.py at main"
[3]: https://kellerjordan.github.io/posts/muon/ "Muon: An optimizer for hidden layers in neural networks | Keller Jordan blog"
[4]: https://nhigham.com/2020/07/28/what-is-the-polar-decomposition/comment-page-1/?utm_source=chatgpt.com "What is the Polar Decomposition? - Nick Higham"
[5]: https://arxiv.org/pdf/2506.10935?utm_source=chatgpt.com "Accelerating Newton-Schulz Iteration for Orthogonalization ..."
[6]: https://github.com/KellerJordan/modded-nanogpt "GitHub - KellerJordan/modded-nanogpt: NanoGPT (124M) in 3 minutes"
[7]: https://arxiv.org/html/2505.16932v2?utm_source=chatgpt.com "The Polar Express: Optimal Matrix Sign Methods and Their ..."
[8]: https://jiechenjiechen.github.io/pub/sign.pdf?utm_source=chatgpt.com "A Stable Scaling of Newton-Schulz for Improving the Sign ..."
