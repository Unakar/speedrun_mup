# Base configuration for GPT-Small scale experiments
name: "gpt_small_base"
description: "Base GPT-Small configuration for MuP experiments"
out_dir: "./outputs/gpt_small"

# Training configuration
training:
  learning_rate: 3e-4
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.1
  grad_clip: 1.0
  
  warmup_iters: 2000
  max_iters: 10000
  lr_decay_iters: 10000
  min_lr: 3e-5
  
  batch_size: 8
  micro_batch_size: 2
  block_size: 1024
  
  dropout: 0.0
  dtype: "bfloat16"
  compile_model: true
  
  save_interval: 2000
  eval_interval: 1000
  log_interval: 100

# Data configuration  
data:
  dataset: "fineweb"
  tokenizer: "gpt2"
  vocab_size: 50304  # Padded to multiple of 128
  
  num_workers: 4
  prefetch_factor: 2
  
  train_split: 0.99
  val_split: 0.01
  
  use_cached: true

# Logging configuration
logging:
  use_wandb: true
  wandb_project: "speedrun-mup"
  wandb_tags: ["gpt-small", "base", "mup"]
  
  log_dir: "./logs"
  save_logs: true
  
  # Standard metrics
  log_train_loss: true
  log_val_loss: true
  log_grad_norm: true
  log_weight_norm: true
  log_lr: true
  log_tokens_per_sec: true
  
  # MuP-specific metrics
  log_activations: true
  log_coordinates: true
  log_spectral: false  # Disabled by default (expensive)
  
  # Frequencies
  log_interval: 100
  activation_log_interval: 1000
  spectral_log_interval: 5000

# Reproducibility
seed: 1337
deterministic: true

# Hardware
device: "cuda"
backend: "nccl"