# GPT-Small with MuP configuration
# Based on modded-nanogpt defaults with MuP scaling

model:
  model_dim: 768
  num_heads: 6      # Original modded-nanogpt default
  num_layers: 12
  vocab_size: 50257  # Original GPT-2 vocab size (gets padded to 50304)
  max_seq_len: 1024
  
  # Architecture features
  use_fp8: true
  use_flex_attention: true
  use_skip_connections: true
  num_value_embeds: 3

training:
  batch_size: 8
  seq_len: 1024
  learning_rate: 3e-4
  weight_decay: 0.1
  grad_clip: 1.0
  max_iters: 10000
  warmup_iters: 2000

mup:
  use_mup: true
  base_width: 768
  target_width: 768
  init_std: 0.02

logging:
  log_interval: 100
  save_interval: 2000
  use_wandb: false
  log_dir: "./logs"

system:
  device: "cuda"
  compile: false
  seed: 1337